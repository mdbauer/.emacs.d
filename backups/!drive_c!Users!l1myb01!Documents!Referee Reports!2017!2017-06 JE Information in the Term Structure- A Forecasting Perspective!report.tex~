% Created 2017-06-30 Fri 14:23
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage[left=3cm,right=3cm,top=2.5cm,bottom=3cm,verbose]{geometry}
\usepackage{setspace}
\onehalfspacing
\date{}
\title{Referee Report on JE Manuscript 2017243 "Information in the Term Structure: A Forecasting Perspective"}
\hypersetup{
 pdfauthor={},
 pdftitle={Referee Report on JE Manuscript 2017243 "Information in the Term Structure: A Forecasting Perspective"},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.1.1 (Org mode 9.0.3)},
 pdflang={English}}
\begin{document}

\maketitle

\section{Summary}
\label{sec:orge6ed3ed}

This paper proposes a new way to estimate dynamic term structure
models (DTSMs) and investigates its benefits in out-of-sample (OOS)
performance and its implications for in-sample fit, estimated factors
and the term premium. The baseline model is a maximally-flexible
affine Gaussian DTSM with factors that are linear combinations of
yields. The standard estimation of this model, following
Joslin-Singleton-Zhou (JSZ), is to choose the linear combinations
(\(W\)) as the principal components of yields, and to obtain maximum
likelihood estimates with a convenient two-step procedure: first, OLS
gives the VAR parameters which minimize one-step ahead forecast errors
(the intercept and mean-reversion matrix, denoted together as
\(\theta^P\)), and second, the likelihood is maximized over the
remaining parameters (\(\theta^Q\)), which mainly determine the
cross-sectional fit, i.e., minimize cross-sectional fitting
errors.

The authors compare the results for the standard approach with
estimates obtained using a new approach that is fundamentally
different: First, they don't treat the linear combinations \(W\) as
fixed but estimate them. Second, they don't use the likelihood
function for estimation, which combines one-step ahead forecast errors for the
\emph{factors} and fitting errors for \emph{yields}, but instead use as a loss
function the average \(k\) -step-ahead forecast errors of yields. They
consider different values of \(k\), depending on the horizon of interest
in a subsequent OOS forecast accuracy evaluation. Estimation is
carried out by numerically minimizing the loss function over \(W\),
\(\theta^P\) and \(\theta_Q\), using an iterative procedure.

The main findings of the paper are that the out-of-sample forecasting
performance is better for the new estimation approach, and that the
chosen linear combinations of yields differ notably from principal
component loadings.

\section{Evaluation}
\label{sec:orgf5bcdc2}

The literature on interest rate modeling has made a tremendous amount
of progress since the 1990s, but accurate forecasting has proven
elusive; in particular the simple no-change (random walk) forecast has
been surprisingly hard to beat (see Duffee, 2013, Handbook of Economic
Forecasting). Hence, making further progress in forecasting methods
for the yield curve is a valuable goal. While the present paper
proposes a new approach that may be useful in certain instances, it
does not make substantial progress relative to what is already well
known in this literature. In particular we know that (a) additional lags of
interest rates matter for forecasting, that is, interest rates are
probably not first-order Markov (e.g., Cochrane and Piazzesi, 2005; Joslin, Le, Singleton, 2013, Journal
of Financial Econometrics; Fenou and Fontaine, 2017, Management
Science), and (b) different linear combinations of yields matter for
fitting the cross section of yields and for forecasting future yields
(e.g., Cochrane and Piazzesi, 2005, 2008; Duffee, 2011, RFS). The
proposed estimation approach combines these two elements, and
unsurprisingly finds gains in forecasting accuracy. And of course the
finding that the third factor looks like the fourth principal
component just reflects the same pattern in yield data that was
documented by Cochrane and Piazzesi. So the paper does
not reveal anything new about the dynamics of yields. The
methodological contribution---considering an alternative loss
function than the likelihood function---could be helpful for some
purposes but this is a marginal contribution, in particular in light
of the fact that this substantially complicates the estimation in a
number of ways. The paper also confuses several issues that are really
separate---namely the points (a) and (b) above as well as the stability
between in-sample estimation and out-of-sample evaluation, all of
which play different roles for forecasting accuracy.

One important improvement would be to strive for more of an
apples-to-apples comparison between estimation approaches. Currently
the estimation differs in many different ways from standard JSZ
estimation. The authors should only change one thing at a time, for
example, only modify the likelihood function by considering
\$k\$-step-ahead forecast errors of the yield factors instead of
one-ste-ahead errors. This particular modification would then still
allow for OLS estimation of the VAR parameters. The estimation of \(W\)
is a wholly different issue, and should be treated separately, if it
belongs in the paper at all. I disagree with the claim that "this
procedure is not of interest in itself" (p.\textasciitilde{}28), because I suppose
that it is in fact allowing for different W than principal components
that is responsible for the forecast gains. At the very least, if the
authors keep their preferred loss function, the paper needs to include
results with and without estimation of \(W\).

In the evaluation of forecast accuracy, the authors thus need to
include results for the standard model, for approaches that vary only
one dimension at a time, and for their preferred approach. This way
the reader can see the contribution of each alteration of the standard
approach. Furthermore, the authors also need to include the random
walk benchmark, i.e., a no-change forecast for each yield, which is
the gold standard in this literature.

A number of questions and criticisms arise about the proposed
estimation procedure, described only in a confusingly written Appendix
B, which the authors need to address. First, it is entirely ad hoc
without any sense of optimality and without discussion of
identicication. For example, it is an open question whether one can
allow for estimation of \(W\) and still have a globally identified
model. Another confusing point is that normally, parameters are
estimated by optimizing both cross-sectional fit and time series fit
(see the P- and Q-likelihood contributions of JSZ), but the approach
proposed in the paper only considers time series fit (for yields
instead of factors). Why is the estimation not based on a criterion
that includes both dimensions? An additional problem is that different
factors and parameters are chosen for each value of \(k\). Which results
should one use to estimate the term premium and expectations
components in yields? Or should the model only be used for
out-of-sample forecasting? Maybe another loss function that averages
across forecasting horizons (and possibly including the
cross-sectional fit) would be more desirable. Finally, the proposed
approach would seem to be very computationally costly, and finding the
global minimum for the loss function would seem to be very difficult
given the very large parameter space over \(W\), \(\theta^P\) and
\(\theta^Q\). Even conditional on \(W\) estimation has been found
difficult in practice. The paper needs a discussion of this issue, and
ideally a simplification of the estimation approach.

\section{Smaller points}
\label{sec:org6807d2a}

\begin{itemize}
\item Equation (2.1), which only includes fitting errors, does not
describe the loss function used in term structure estimation or
JSZ---the likelihood considers both fitting errors and forecast
errors. The discussion in this section and later on is
misleading. Only from the appendix did I gather that the authors are
in fact comparing standard JSZ estimation with their involved
estimation approach. The exposition needs to be clarified. I also
recommend first writing down a model before turning to estimation
and loss functions. The authors also need to write down the
likelihood function.
\item The yield data and sample period are quite unusual. I recommend
using the standard Gurkaynak-Sack-Wright yields from the Federal
Reserve, possibly combined with T-bill rates at the short end. The
authors should strive to demonstrate that the results are robust for
different yield data sources and sample periods.
\item Footnote 10 is completely repeated from the introduction.
\item The estimation approach proposed in the paper needs to be discussed
in the main body, not in an appendix.
\item Pages 26 and 27 on JSZ do not add anything relative to the original
JSZ paper and should be deleted.
\end{itemize}
\end{document}
