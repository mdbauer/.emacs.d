\part*{Appendix}
\numberwithin{table}{section} \numberwithin{figure}{section}
\singlespacing

\section{Derivations of theoretical results}

\subsection{Derivations for Section \ref{sec:intuition}}
\label{app:intuition}

Let $y=(y_{1+h},y_{2+h},...,y_{T+h})^{\prime}$ and stack $x_{1t}^{\prime}$ and
$x_{2t}^{\prime}$ into $(T\times K_{1})$ and $(T\times K_{2})$ matrices
denoted $X_{1}$ and $X_{2}$. \ Note that the OLS estimates of equation
\eqref{eq:pred} satisfy%
\[
\left[
\begin{array}
[c]{cc}%
X_{1}^{\prime}X_{1} & X_{1}^{\prime}X_{2}\\
X_{2}^{\prime}X_{1} & X_{2}^{\prime}X_{2}%
\end{array}
\right]  \left[
\begin{array}
[c]{c}%
b_{1}\\
b_{2}%
\end{array}
\right]  =\left[
\begin{array}
[c]{c}%
X_{1}^{\prime}y\\
X_{2}^{\prime}y
\end{array}
\right]  .
\]
Premultiply the first row by $X_{2}^{\prime}X_{1}(X_{1}^{\prime}X_{1})^{-1}$
and subtract the result from the second,%
\[
(X_{2}^{\prime}M_{1}X_{2})b_{2}=X_{2}^{\prime}M_{1}y,
\]
for $M_{1}=I_{T}-X_{1}(X_{1}^{\prime}X_{1})^{-1}X_{1}^{\prime}.$ \ Using the
fact that $M_{1}$ is symmetric and idempotent we have
\begin{equation}
X_{2}^{\prime}M_{1}X_{2}=(M_{1}X_{2})^{\prime}M_{1}X_{2}=\sum\tilde{x}%
_{2t}\tilde{x}_{2t}^{\prime} \label{eq: idempotent}%
\end{equation}%
\begin{equation}
b_{2}=\left(  \sum\tilde{x}_{2t}\tilde{x}_{2t}^{\prime}\right)  ^{-1}\left(
\sum\tilde{x}_{2t}y_{t+h}\right)  . \label{eq: frisch}%
\end{equation}
Substituting equation \eqref{eq:pred} into (\ref{eq: frisch}) and using the
facts that $\sum\tilde{x}_{2t}x_{1t}^{\prime}=0$ (by the orthogonality
property of residuals) and that $\sum\tilde{x}_{2t}x_{2t}^{\prime}=\sum
\tilde{x}_{2t}\tilde{x}_{2t}^{\prime}$ (again by idempotence of $M_{1})$
gives
\begin{equation}
b_{2}=\beta_{2}+\left(  \sum\tilde{x}_{2t}\tilde{x}_{2t}^{\prime}\right)
^{-1}\left(  \sum\tilde{x}_{2t}u_{t+h}\right)  \label{eq: b2 and beta2}%
\end{equation}
from which the Wald test is%
\begin{align*}
&  (b_{2}-\beta_{2})^{\prime}s^{-2}%
%TCIMACRO{\tsum \nolimits_{t=1}^{T}}%
%BeginExpansion
{\textstyle\sum\nolimits_{t=1}^{T}}
%EndExpansion
\tilde{x}_{2t}\tilde{x}_{2t}^{\prime}(b_{2}-\beta_{2})\\
&  =\left(  \sum\nolimits_{t=1}^{T}u_{t+1}\tilde{x}_{2t}^{\prime}\right)
\left(  s^{2}\sum\nolimits_{t=1}^{T}\tilde{x}_{2t}\tilde{x}_{2t}^{\prime
}\right)  ^{-1}\left(  \sum\nolimits_{t=1}^{T}\tilde{x}_{2t}u_{t+1}\right)
\end{align*}
as claimed in (\ref{eq:wald stat})

Note that if $u|X_{1},X_{2}\sim N(0,\sigma_{u}^{2}I_{T})$, then $K_{2}^{-1}$
times expression (\ref{eq:wald stat}) would have an exact $F(K_{2}%
,T-K_{1}-K_{2})$ distribution for every sample size $T$ and any stationary or
nonstationary process for $x_{2t}.$ \ Under the weaker assumption that
$E(u_{t+1}|x_{t},x_{t-1},...,x_{1})=0$ but $E(u_{t}|x_{t},x_{t-1}%
,...,x_{1})\neq0,$ the Wald statistic (\ref{eq:wald stat}) will still be
asymptotically $\chi^{2}(K_{2})$ under standard first-order stationary
asymptotics, as can be seen from equation (\ref{eq: diff SSR1}) below for the
special case $h=1$ and $S=\sigma_{u}^{2}Q.$ \ The problems arise when $x_{1t}$
is correlated with $u_{t}$ and furthermore $x_{t}$ is highly persistent. \ In
the case of unit-root processes these problems give (\ref{eq:wald stat}) an
asymptotic distribution that is not $\chi^{2}(K_{2})$, and for near-unit-root
processes they cause the small-sample distribution to be quite different from
a $\chi^{2}(K_{2})$.

The unit-root derivations this next paragraph assume a functional central
limit theorem $T^{-1/2}x_{i,[T\lambda]}\Rightarrow B_{i}(\lambda)$ for $i=1,2$
with $0\leq\lambda\leq1$, $[T\lambda]$ the largest integer less than or equal
to $T\lambda,$ $B_{i}(\lambda)$ $K_{i}$-dimensional Brownian motion, and
$\Rightarrow$ denoting weak convergence in probability measure. \ From the
FCLT and the Continuous Mapping Theorem,
\begin{align*}
\hat{A}_{T}  &  =\left[  T^{-1}\int_{0}^{1}x_{2,[T\lambda]}x_{1,[T\lambda
]}^{\prime}d\lambda\right]  \left[  T^{-1}\int_{0}^{1}x_{1,[T\lambda
]}x_{1,[T\lambda]}^{\prime}d\lambda\right]  ^{-1}\\
&  \Rightarrow\left[  \int_{0}^{1}B_{2}(\lambda)B_{1}(\lambda)^{\prime
}d\lambda\right]  \left[  \int_{0}^{1}B_{1}(\lambda)B_{1}(\lambda)^{\prime
}d\lambda\right]  ^{-1}\\
&  \equiv\tilde{A}.
\end{align*}
\ Notice that%
\begin{align}
\sum\nolimits_{t=1}^{T}\tilde{x}_{2t}u_{t+1}  &  =\sum\nolimits_{t=1}%
^{T}x_{2t}u_{t+1}-\hat{A}_{T}\sum\nolimits_{t=1}^{T}x_{1t}u_{t+1}\nonumber\\
&  =\sum\nolimits_{t=1}^{T}x_{2t}u_{t+1}-\sum\nolimits_{t=1}^{T}x_{2t}%
x_{1t}^{\prime}Z_{T} \label{eq: DF type eq}%
\end{align}
for $Z_{T}=\left(  \sum\nolimits_{t=1}^{T}x_{1t}x_{1t}^{\prime}\right)
^{-1}\left(  \sum\nolimits_{t=1}^{T}x_{1t}u_{t+1}\right)  $ \ If $x_{1t}$ is a
unit-root process that is correlated with the lag of $u_{t+1},$ $Z_{T}$ will
have a nonstandard distribution. \ For example, if $x_{1t}$ is a scalar random
walk with $x_{1,t+1}=x_{1t}+u_{t+1},$ then $Z_{T}$ has the same distribution
as $\hat{\rho}_{T}-1$ where $\hat{\rho}_{T}$ is the OLS coefficient from a
regression of $x_{1,t+1}$ on $x_{1t},$ a distribution with a negative bias
that is well-known from unit root regressions.\footnote{See for example
\citet[][eq {[}17.4.7{]}]{hamilton}} \ If $x_{2t}$ is uncorrelated with
$x_{1t},$ then unlike the Dicky-Fuller distribution, the second term in
(\ref{eq: DF type eq}) is symmetric around zero and is uncorrelated with the
first term, so that the variance of $\sum\nolimits_{t=1}^{T}\tilde{x}%
_{2t}u_{t+1}$ is strictly greater than that of $\sum\nolimits_{t=1}^{T}%
x_{2t}u_{t+1}.$

\subsection{Derivations for Section \ref{sec:sebias}}
\label{app:example}

For our local-to-unity results we assume as in \citet[][eq (2.17)]{stock-94}
that $T^{-1/2}x_{i,[T\lambda]}\Rightarrow\sigma_{i}J_{c_{i}}(\lambda).$ \ We
first note from \citet[][Lemma 3.1(d)]{phillips-88} that
\[
T^{-2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
(x_{1t}-\bar{x}_{1})^{2}\Rightarrow\sigma_{1}^{2}\left\{  \int_{0}%
^{1}[J_{c_{1}}(\lambda)]^{2}d\lambda-\left[  \int_{0}^{1}[J_{c_{1}}%
(\lambda)]d\lambda\right]  ^{2}\right\}  =\sigma_{1}^{2}\int[J_{c_{1}}^{\mu
}]^{2}%
\]
where in the sequel our notation suppresses the dependence on $\lambda$ and
lets $\int$ denote integration over $\lambda$ from 0 to 1. \ The analogous
operation applied to the numerator of (\ref{A asymptotics}) yields%
\[
A_{T}=\frac{T^{-2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
(x_{1t}-\bar{x}_{1})(x_{2t}-\bar{x}_{2})}{T^{-2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
(x_{1t}-\bar{x}_{1})^{2}}\Rightarrow\frac{\sigma_{1}\sigma_{2}\int J_{c_{1}%
}^{\mu}J_{c_{2}}^{\mu}}{\sigma_{1}^{2}\int[J_{c_{1}}^{\mu}]^{2}}%
\]
as claimed in (\ref{A asymptotics}). $\ $Also%
\[
T^{-1/2}\bar{x}_{2}=T^{-3/2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{2t}=\int_{0}^{1}T^{-1/2}x_{2,[T\lambda]}d\lambda\Rightarrow\sigma_{2}%
\int_{0}^{1}J_{c_{2}}(\lambda)d\lambda.
\]
Since $\tilde{x}_{2t}=x_{2t}-\bar{x}_{2}-A_{T}(x_{1t}-\bar{x}_{1}),$%
\begin{align*}
T^{-1/2}\tilde{x}_{2,[T\lambda]}  &  \Rightarrow\sigma_{2}\left\{  J_{c_{2}%
}(\lambda)-\int_{0}^{1}J_{c_{2}}(s)ds-A\left[  J_{c_{1}}(\lambda)-\int_{0}%
^{1}J_{c_{1}}(s)ds\right]  \right\} \\
&  =\sigma_{2}\left\{  J_{c_{2}}^{\mu}(\lambda)-AJ_{c_{1}}^{\mu}%
(\lambda)\right\}  =\sigma_{2}K_{c_{1},c_{2}}(\lambda)
\end{align*}%
\begin{equation}
T^{-2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}^{2}=\int_{0}^{1}\{T^{-1/2}\tilde{x}_{2,[T\lambda]}%
\}^{2}d\lambda\Rightarrow\sigma_{2}^{2}\int_{0}^{1}\left\{  K_{c_{1},c_{2}%
}(\lambda)\right\}  ^{2}d\lambda. \label{x2 tilde J asymptotics}%
\end{equation}
\qquad

Note we can write%
\[
\left[
\begin{array}
[c]{c}%
\varepsilon_{1t}\\
\varepsilon_{2t}\\
u_{t}%
\end{array}
\right]  =\left[
\begin{array}
[c]{ccc}%
\sigma_{1} & 0 & 0\\
0 & \sigma_{2} & 0\\
\delta\sigma_{u} & 0 & \sqrt{1-\delta^{2}}\sigma_{u}%
\end{array}
\right]  \left[
\begin{array}
[c]{c}%
v_{1t}\\
v_{2t}\\
v_{0t}%
\end{array}
\right]
\]
where $(v_{1t},v_{2t},v_{0t})^{\prime}$ is a martingale-difference sequence
with unit variance matrix. \ From Lemma 3.1(e) in \cite{phillips-88} we see%
\begin{align}
T^{-1}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}u_{t+1}  &  =T^{-1}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
[x_{2t}-\bar{x}_{2}-A_{T}(x_{1t}-\bar{x}_{1})](\delta\sigma_{u}v_{1,t+1}%
+\sqrt{1-\delta^{2}}\sigma_{u}v_{0,t+1})\nonumber\\
&  \Rightarrow\delta\sigma_{2}\sigma_{u}\int K_{c_{1},c_{2}}dW_{1}%
+\sqrt{1-\delta^{2}}\sigma_{2}\sigma_{u}\int K_{c_{1},c_{2}}dW_{0}.
\label{x2tilde times ut asymptotics}%
\end{align}
Recalling (\ref{eq:wald stat}), the $t$-test of a true null hypothesis about
$\beta_{2}$ can be written as
\begin{equation}
\tau=\frac{%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}u_{t+1}}{\left\{  s^{2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}^{2}\right\}  ^{1/2}}=\frac{T^{-1}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}u_{t+1}}{\left\{  s^{2}T^{-2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}^{2}\right\}  ^{1/2}} \label{t stat}%
\end{equation}
where%
\begin{equation}
s^{2}\overset{p}{\rightarrow}\sigma_{u}^{2}. \label{s2 plim}%
\end{equation}
Substituting (\ref{s2 plim}), (\ref{x2tilde times ut asymptotics}), and
(\ref{x2 tilde J asymptotics}) into (\ref{t stat}) produces%
\[
\tau\Rightarrow\frac{\sigma_{2}\sigma_{u}\left\{  \delta\int K_{c_{1},c_{2}%
}dW_{1}+\sqrt{1-\delta^{2}}\int K_{c_{1},c_{2}}dW_{0}\right\}  }{\left\{
\sigma_{u}^{2}\sigma_{2}^{2}\int(K_{c_{1},c_{2}})^{2}\right\}  ^{1/2}}%
\]
as claimed in (\ref{b2 t stat}).

Last we demonstrate that the variance of $Z_{1}$ exceeds unity. We can write%
\begin{equation}
Z_{1}=\frac{\int_{0}^{1}J_{c_{2}}^{\mu}(\lambda)dW_{1}(\lambda)}{\left\{
\int_{0}^{1}[K_{c_{1},c_{2}}(\lambda)]^{2}d\lambda\right\}  ^{1/2}}%
-\frac{A\int_{0}^{1}J_{c_{1}}^{\mu}(\lambda)dW_{1}(\lambda)}{\left\{  \int
_{0}^{1}[K_{c_{1},c_{2}}(\lambda)]^{2}d\lambda\right\}  ^{1/2}}
\label{delta = 1 asymptotics}%
\end{equation}
Consider the denominator in these expressions, and note that%
\begin{align*}
\int_{0}^{1}[J_{c_{2}}^{\mu}(\lambda)]^{2}d\lambda &  =\int_{0}^{1}[J_{c_{2}%
}^{\mu}(\lambda)-AJ_{c_{1}}^{\mu}(\lambda)+AJ_{c_{1}}^{\mu}(\lambda
)]^{2}d\lambda\\
&  =\int_{0}^{1}[K_{c_{1},c_{2}}(\lambda)]^{2}d\lambda+\int_{0}^{1}[AJ_{c_{1}%
}^{\mu}(\lambda)]^{2}d\lambda\\
&  >\int_{0}^{1}[K_{c_{1},c_{2}}(\lambda)]^{2}d\lambda
\end{align*}
where the cross-product term dropped out in the second equation by the
definition of $A$ in (\ref{A asymptotics})$.$ \ This means that the following
inequality holds for all realizations:%
\begin{equation}
\left\vert \frac{\int_{0}^{1}J_{c_{2}}^{\mu}(\lambda)dW_{1}(\lambda)}{\left\{
\int_{0}^{1}[K_{c_{1},c_{2}}(\lambda)]^{2}d\lambda\right\}  ^{1/2}}\right\vert
>\left\vert \frac{\int_{0}^{1}J_{c_{2}}^{\mu}(\lambda)dW_{1}(\lambda
)}{\left\{  \int_{0}^{1}[J_{c_{2}}^{\mu}(\lambda)]^{2}d\lambda\right\}
^{1/2}}\right\vert . \label{delta = 1 inequality}%
\end{equation}
Adapting the argument made in footnote \ref{fn:normal}, the magnitude inside
the absolute-value operator on the right side of (\ref{delta = 1 inequality})
can be seen to have a $N(0,1)$ distribution. Inequality
\eqref{delta = 1 inequality} thus establishes that the first term in
\eqref{delta = 1 asymptotics} has a variance that is greater than unity. The
second term in (\ref{delta = 1 asymptotics}) turns out to be uncorrelated with
the first, and hence contributes additional variance to $Z_{1}$, although we
have found that the first term appears to be the most important
factor.\footnote{These claims are based on moments of the respective
functionals as estimated from discrete approximations to the
Ornstein-Uhlenbeck processes.} In sum, these arguments show that
$\text{Var}(Z_{1})>1$.

\subsection{Derivations for Section \ref{sec:trends}}
\label{app:trends}

First consider the case when $\rho_{1}=\rho_{2}=1,$ $\mu_{1}=0,$ $\mu_{2}%
\neq0,$ and $Corr(\varepsilon_{1t},u_{t})=1.$ \ Then $T^{-1/2}x_{1,[T\lambda
]}\Rightarrow\sigma_{1}W_{1}(\lambda)$ for $W_{1}(\lambda)$ standard Brownian
motion, $T^{-1/2}%
%TCIMACRO{\tsum \nolimits_{t=1}^{T}}%
%BeginExpansion
{\textstyle\sum\nolimits_{t=1}^{T}}
%EndExpansion
u_{t+1}\Rightarrow\sigma_{1}W_{1}(1),$ while $x_{2t}=\mu_{2}t+%
%TCIMACRO{\tsum \nolimits_{s=1}^{t}}%
%BeginExpansion
{\textstyle\sum\nolimits_{s=1}^{t}}
%EndExpansion
\varepsilon_{2s}$ gives $T^{-1}x_{2,[T\lambda]}\Rightarrow\mu_{2}\lambda$ as
in \citet[][pp.~495-497]{hamilton}. \ Let $x_{t}=(1,x_{1t},x_{2t})^{\prime}$
so $b=\beta+\left(
%TCIMACRO{\tsum \nolimits_{t=1}^{T}}%
%BeginExpansion
{\textstyle\sum\nolimits_{t=1}^{T}}
%EndExpansion
x_{t}x_{t}^{\prime}\right)  ^{-1}%
%TCIMACRO{\tsum \nolimits_{t=1}^{T}}%
%BeginExpansion
{\textstyle\sum\nolimits_{t=1}^{T}}
%EndExpansion
x_{t}u_{t+1}.$ \ Define%
\[
\Upsilon_{T}=\left[
\begin{array}
[c]{ccc}%
T^{1/2} & 0 & 0\\
0 & T & 0\\
0 & 0 & T^{3/2}%
\end{array}
\right]  .
\]
Then very similar algebra to that in \citet[][pp.~498-500]{hamilton} gives%
\begin{align*}
\Upsilon_{T}(b-\beta)  &  =\left[  \Upsilon_{T}^{-1}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{t}x_{t}^{\prime}\Upsilon_{T}^{-1}\right]  ^{-1}\left[  \Upsilon_{T}^{-1}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{t}u_{t+1}\right] \\
&  \Rightarrow\left[
\begin{array}
[c]{ccc}%
1 & \sigma_{1}\int W_{1}(\lambda) & \mu_{2}/2\\
\sigma_{1}\int W_{1}(\lambda) & \sigma_{1}^{2}\int[W_{1}(\lambda)]^{2} &
\mu_{2}\sigma_{1}\int\lambda W_{1}(\lambda)\\
\mu_{2}/2 & \mu_{2}\sigma_{1}\int\lambda W_{1}(\lambda) & \mu_{2}^{2}/3
\end{array}
\right]  ^{-1}\left[
\begin{array}
[c]{c}%
\sigma_{1}W_{1}(1)\\
(1/2)\sigma_{1}^{2}[W^{2}(1)-1]\\
\mu_{2}\sigma_{1}[W_{1}(1)-\int W_{1}(\lambda)]
\end{array}
\right] \\
&  =\left[
\begin{array}
[c]{ccc}%
\sigma_{1} & 0 & 0\\
0 & 1 & 0\\
0 & 0 & \sigma_{1}/\mu_{2}%
\end{array}
\right]  \left[
\begin{array}
[c]{ccc}%
1 & \int W_{1}(\lambda) & 1/2\\
\int W_{1}(\lambda) & \int[W_{1}(\lambda)]^{2} & \int\lambda W_{1}(\lambda)\\
1/2 & \int\lambda W_{1}(\lambda) & 1/3
\end{array}
\right]  ^{-1}\left[
\begin{array}
[c]{c}%
W_{1}(1)\\
(1/2)[W^{2}(1)-1]\\
W_{1}(1)-\int W_{1}(\lambda)
\end{array}
\right]  .
\end{align*}
Observe that the middle element, $T(b_{1}-\beta_{1})$ is the identical
distribution as that of $T(\hat{\rho}-1)$ in the Case 4 unit root distribution
in \citet[][p.~499]{hamilton}, and the $t$-statistic $(b_{1}-\beta_{1}%
)/\hat{\sigma}_{b_{1}}$ is identical to the Case 4 Dickey-Fuller $t$ statistic (\citet[][eq {[}17.4.55{]}]{hamilton}).

Consider next the case when $\rho_{1}=\rho_{2}=1,$ $\mu_{1}\neq0,$ $\mu
_{2}\neq0,$ $Corr(\varepsilon_{1t},u_{t})=1,$ and $Corr(\varepsilon
_{1t},\varepsilon_{2s})=0$ for all $s$. \ Let's evaluate first the
characteristics of a transformed regression of $y_{t+1}$ on $\tilde{x}%
_{t}=Hx_{t}$ for%
\[
H=\left[
\begin{array}
[c]{ccc}%
1 & 0 & 0\\
0 & 1 & -\mu_{1}/\mu_{2}\\
0 & 0 & 1
\end{array}
\right]
\]%
\[
\tilde{b}=(%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{t}\tilde{x}_{t}^{\prime})^{-1}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{t}y_{t+1}=(H^{\prime})^{-1}b
\]%
\[
\tilde{\beta}=(H^{\prime})^{-1}\beta.
\]
Then%
\begin{align*}
\tilde{x}_{1t}  &  =x_{1t}-(\mu_{1}/\mu_{2})x_{2t}\\
&  =\mu_{1}t+%
%TCIMACRO{\tsum \nolimits_{s=1}^{t}}%
%BeginExpansion
{\textstyle\sum\nolimits_{s=1}^{t}}
%EndExpansion
\varepsilon_{1s}-(\mu_{1}/\mu_{2})\left(  \mu_{2}t+%
%TCIMACRO{\tsum \nolimits_{s=1}^{t}}%
%BeginExpansion
{\textstyle\sum\nolimits_{s=1}^{t}}
%EndExpansion
\varepsilon_{2s}\right) \\
&  =%
%TCIMACRO{\tsum \nolimits_{s=1}^{t}}%
%BeginExpansion
{\textstyle\sum\nolimits_{s=1}^{t}}
%EndExpansion
\varepsilon_{1s}-(\mu_{1}/\mu_{2})%
%TCIMACRO{\tsum \nolimits_{s=1}^{t}}%
%BeginExpansion
{\textstyle\sum\nolimits_{s=1}^{t}}
%EndExpansion
\varepsilon_{2s}%
\end{align*}
and%
\begin{align*}
T^{-1/2}\tilde{x}_{1,[T\lambda]}  &  \Rightarrow\sigma_{1}W_{1}(\lambda
)-(\mu_{1}/\mu_{2})\sigma_{2}W_{2}(\lambda)\\
&  \equiv\kappa(\lambda)
\end{align*}%
\[
\Upsilon_{T}(\tilde{b}-\tilde{\beta})\Rightarrow\left[
\begin{array}
[c]{ccc}%
1 & \int\kappa(\lambda) & \mu_{2}/2\\
\int\kappa(\lambda) & \int[\kappa(\lambda)]^{2} & \mu_{2}\int\lambda
\kappa(\lambda)\\
\mu_{2}/2 & \mu_{2}\int\lambda\kappa(\lambda) & \mu_{2}^{2}/3
\end{array}
\right]  ^{-1}\left[
\begin{array}
[c]{c}%
\sigma_{1}W_{1}(1)\\
\sigma_{1}\int\kappa(\lambda)dW_{1}\\
\mu_{2}\sigma_{1}[W_{1}(\lambda)-\int W_{1}(\lambda)]
\end{array}
\right]  .
\]
The middle element, $T(\tilde{b}_{1}-\beta_{1}),$ has a distribution that
approaches the Dickey-Fuller Case 4 as $\sigma_{2}\rightarrow0$ and is a
related unit-root distribution for general $\sigma_{2}>0.$

Translating back in terms of the original regression, we have $b=H^{\prime
}\tilde{b},$ $b_{1}=\tilde{b}_{1},$
\[
b_{2}=\tilde{b}_{2}-(\mu_{1}/\mu_{2})\tilde{b}_{1}=\tilde{b}_{2}-(\mu_{1}%
/\mu_{2})b_{1}%
\]%
\begin{align*}
T(b_{2}-\beta_{2})  &  =T(\tilde{b}_{2}-\tilde{\beta}_{2})-(\mu_{1}/\mu
_{2})T(b_{1}-\beta_{1})\\
&  \Rightarrow0-(\mu_{1}/\mu_{2})T(b_{1}-\beta_{1})
\end{align*}
since $T^{3/2}(\tilde{b}_{2}-\tilde{\beta}_{2})\sim O_{p}(1).$ \ Thus
$b_{2}-\beta_{2}$ has the same asymptotic distribution as $-(\mu_{1}/\mu
_{2})(b_{1}-\beta_{1}),$ with $t$-tests on either $b_{1}$ or $b_{2}$ having a
distribution related to the Dickey-Fuller Case 4. \ When $x_{1t}$ and $x_{2t}$
share the same trend $(\mu_{1}=\mu_{2})$, the distribution of $b_{2}$ will
simply be the negative of that of $b_{1}.$

By contrast, if we were to regress $y_{t+1}=\beta_{0}+\beta_{1}x_{1t}+u_{t+1}$
on $x_{1t}$ alone, or $y_{t+1}=\beta_{0}+\beta_{2}x_{2t}+u_{t+1}$ on $x_{2t}$
alone, $t$-tests on $\beta_{1}$ or $\beta_{2}$ would be asymptotically
$N(0,1)$, from the same algebra as in \citet[][pp.~495-497]{hamilton}. \ Thus
for example if the true $\beta_{1}\neq0$ and $\beta_{2}=0,$ when we do the
regression on $x_{1t}$ alone we would have perfectly appropriate tests about
$\beta_{1},$ but if we add $x_{2t}$ to the regression, tests about both
$\beta_{1}$ and $\beta_{2}$ become distorted and $x_{2t}$ could spuriously
appear to be helpful in improving the estimate of $\beta_{1}.$

\subsection{Derivations for Section \ref{sec:overlapping}}
\label{app:overlapping}

Note from (\ref{eq: idempotent}) that%
\[%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}\tilde{x}_{2t}^{\prime}=%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{2t}x_{2t}^{\prime}-\left(
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{2t}x_{1t}^{\prime}\right)  \left(
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{1t}x_{1t}^{\prime}\right)  ^{-1}\left(
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{1t}x_{2t}^{\prime}\right)  .
\]
If $x_{t}$ is covariance-stationary and ergodic for second moments,
\begin{align}
T^{-1}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}\tilde{x}_{2t}^{\prime}  &  =T^{-1}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{2t}x_{2t}^{\prime}-\left(  T^{-1}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{2t}x_{1t}^{\prime}\right)  \left(  T^{-1}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{1t}x_{1t}^{\prime}\right)  ^{-1}\left(  T^{-1}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{1t}x_{2t}^{\prime}\right) \nonumber\\
&  \overset{p}{\rightarrow}E(x_{2t}x_{2t}^{\prime})-E(x_{2t}x_{1t}^{\prime
})\left[  E(x_{1t}x_{1t}^{\prime})\right]  ^{-1}E(x_{1t}x_{2t}^{\prime
})\nonumber\\
&  =E(x_{2t}x_{2t}^{\prime})\equiv Q \label{eq: plim xtilde}%
\end{align}
with the last line following from the assumption that $x_{1t}$ and $x_{2t}$
are uncorrelated. \ From (\ref{eq: b2 and beta2}) we also know%
\begin{equation}
T^{1/2}(b_{2}-\beta_{2})=\left(  T^{-1}\sum\tilde{x}_{2t}\tilde{x}%
_{2t}^{\prime}\right)  ^{-1}\left(  T^{-1/2}\sum\tilde{x}_{2t}u_{t+h}\right)
\label{eq:b2 decomp}%
\end{equation}
where%
\[
T^{-1/2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}u_{t+h}=T^{-1/2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{2t}u_{t+h}-A_{T}T^{-1/2}\left(
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{1t}u_{t+h}\right)  .
\]
But if $E(x_{2t}x_{1t}^{\prime})=0,$ then plim($A_{T})=0,$ meaning%
\[
T^{-1/2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}u_{t+h}\overset{d}{\rightarrow}T^{-1/2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
x_{2t}u_{t+h}.
\]
This will be recognized as $\sqrt{T}$ times the sample mean of a random vector
with population mean zero, for which the Central Limit Theorem would take the
form%
\begin{equation}
T^{-1/2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}u_{t+h}\overset{d}{\rightarrow}r\sim N(0,S) \label{eq: CLT}%
\end{equation}
for $S$ given in (\ref{eq: S def}). \ Combining results (\ref{eq: plim xtilde}%
), (\ref{eq:b2 decomp}) and (\ref{eq: CLT}) gives (\ref{b2 distribution}).

To derive (\ref{diff R2 asymptotic}), let $b=(b_{1}^{\prime},b_{2}^{\prime
})^{\prime}$ denote the OLS coefficients when the regression includes both
$x_{1t}$ and $x_{2t}$ and $b_{1}^{\ast}$ the coefficients from an OLS
regression that includes only $x_{1t}.$ \ The sum of squared residuals from
the latter regression can be written%
\begin{align*}
SSR_{1}  &  =%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
(y_{t+h}-x_{1t}^{\prime}b_{1}^{\ast})^{2}\\
&  =%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
(y_{t+h}-x_{t}^{\prime}b+x_{t}^{\prime}b-x_{1t}^{\prime}b_{1}^{\ast})^{2}\\
&  =%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
(y_{t+h}-x_{t}^{\prime}b)^{2}+%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
(x_{t}^{\prime}b-x_{1t}^{\prime}b_{1}^{\ast})^{2}%
\end{align*}
where all summations are over $t=1,...,T$ \ and the last equality follows from
the orthogonality property of OLS. \ Thus the difference in $SSR$ between the
two regressions is%
\begin{equation}
SSR_{1}-SSR_{2}=%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
(x_{t}^{\prime}b-x_{1t}^{\prime}b_{1}^{\ast})^{2}. \label{SSR diff}%
\end{equation}
It's also not hard to show that the fitted values for the full regression
could be calculated as\footnote{The easiest way to confirm the claim is to
show that the residuals implied by (\ref{fitted values}) satisfy the
orthogonality conditions required of the original full regression, namely,
that they are orthogonal to $x_{1t}$ and $x_{2t}.$ \ That the residual
$y_{t+h}-x_{1t}^{\prime}b_{1}^{\ast}-\tilde{x}_{2t}^{\prime}b_{2}$ is
orthogonal to $x_{1t}$ follows from the fact that $y_{t+h}-x_{1t}^{\prime
}b_{1}^{\ast}$ is orthogonal to $x_{1t}$ by the definition of $b_{1}^{\ast}$
while $\tilde{x}_{2t}$ is orthogonal to $x_{1t}$ by the construction of
$\tilde{x}_{2t}.$ \ Likewise $y_{t+h}-\tilde{x}_{2t}^{\prime}b_{2}$ is
orthogonal to $\tilde{x}_{2t}$ by (\ref{eq: frisch}), and since $x_{1t}$ is
again orthogonal to $\tilde{x}_{2t}$ by the construction of $\tilde{x}_{2t},$
it follows that $y_{t+h}-x_{1t}^{\prime}b_{1}^{\ast}-\tilde{x}_{2t}b_{2}$ is
orthgonal to $\tilde{x}_{2t}.$ \ Since $y_{t+h}-x_{1t}^{\prime}b_{1}^{\ast
}-\tilde{x}_{2t}^{\prime}b_{2}$ is orthogonal to both $x_{1t}$ and $\tilde
{x}_{2t},$ it is also orthogonal to $x_{2t}=\tilde{x}_{2t}+A_{T}x_{1t}$.}%
\begin{equation}
x_{t}^{\prime}b=x_{1t}^{\prime}b_{1}^{\ast}+\tilde{x}_{2t}^{\prime}b_{2}.
\label{fitted values}%
\end{equation}
Thus from (\ref{SSR diff}) and (\ref{fitted values}),%
\[
SSR_{1}-SSR_{2}=%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
(\tilde{x}_{2t}^{\prime}b_{2})^{2}.
\]


If the true value of $\beta_{2}$ is zero, then (\ref{eq: b2 and beta2})
becomes
\begin{equation}
b_{2}=\left(
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}\tilde{x}_{2t}^{\prime}\right)  ^{-1}\left(
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}u_{t+h}\right)  \label{b2 expression}%
\end{equation}%
\begin{align}
SSR_{1}-SSR_{2}  &  =b_{2}^{\prime}\left(
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}\tilde{x}_{2t}^{\prime}\right)  b_{2}\nonumber\\
&  =\left(  T^{-1/2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
u_{t+h}\tilde{x}_{2t}^{\prime}\right)  \left(  T^{-1}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}\tilde{x}_{2t}^{\prime}\right)  ^{-1}\left(  T^{-1/2}%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
\tilde{x}_{2t}u_{t+h}\right)  . \label{diff SSR 2}%
\end{align}
Results (\ref{eq: plim xtilde}) and (\ref{eq: CLT}) then establish
\begin{equation}
SSR_{1}-SSR_{2}\overset{d}{\rightarrow}r^{\prime}Q^{-1}r.
\label{eq: diff SSR1}%
\end{equation}
Recall that $R^{2}$ is defined as%
\[
R^{2}=1-\frac{SSR}{\sum_{t=1}^{T}(y_{t+h}-\bar{y}_{h})^{2}}%
\]
so the difference in $R^{2}$ is
\[
R_{2}^{2}-R_{1}^{2}=\frac{(SSR_{1}-SSR_{2})}{\sum_{t=1}^{T}(y_{t+h}-\bar
{y}_{h})^{2}}.
\]
Thus from (\ref{diff SSR 2}),%
\[
T(R_{2}^{2}-R_{1}^{2})=\frac{(SSR_{1}-SSR_{2})}{%
%TCIMACRO{\tsum }%
%BeginExpansion
{\textstyle\sum}
%EndExpansion
(y_{t+h}-\bar{y}_{h})^{2}/T}\overset{d}{\rightarrow}\frac{r^{\prime}Q^{-1}%
r}{\gamma}%
\]
as claimed in (\ref{diff R2 asymptotic}).

\section{Alternative spanning hypotheses}
\label{app:pcs}

Our baseline version of the spanning hypothesis is that three PCs of
bond yields fully capture the information underlying expected bond
returns and future interest rates, motivated by the well-known fact that three PCs capture
almost all variation in the cross section of yields across maturities
\citep{littermanscheinkman}. But it is possible that higher-order PCs,
while explaining only a miniscule share of cross-sectional variation
of current yields, still contain information about expectations of
future yields (see Section \ref{sec:cp} for tests of this
hypothesis). We therefore investigate
two alternative versions of the spanning hypothesis, in which four or
five PCs of yields span the information in the yield
curve. Application of our bootstrap method to test these hypotheses is
straightforward, since the only change to the approach described in
Section \ref{sec:bootstrap} is that $x_{1t}$ now contains four or five
PCs. We consider these additional null hypotheses for the three
empirical applications in Sections \ref{sec:jps}, \ref{sec:ln} and
\ref{sec:cpo}, where macro variables are proposed as the additional predictors.

\begin{table}
\caption{Increase in $\bar{R}^2$ from addition of macro variables}
\label{tab:pcs1}\centering
\vspace{.5pc}
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{Original sample period} & \multicolumn{3}{c}{Later sample: 1985--2016} \\
\cmidrule(lr{.5em}){2-4} \cmidrule(lr{.5em}){5-7}
& $N=3$ & $N=4$ & $N=5$ & $N=3$ & $N=4$ & $N=5$ \\
  \midrule
  \input{../../tables/pcs1.tex}
  \bottomrule
\end{tabular}%
\par
\begin{flushleft}
  {\small Increase in $\bar{R}^2$ for regressions of annual excess bond returns
    when macro variables are added to a specification that includes
    $N$ PCs of yields. In parentheses are 95\%-bootstrap intervals, obtained under the
    null hypothesis that the macro variables have no predictive
    power. The bootstrap procedure is described in the text.}
\end{flushleft}
\end{table}

In Table \ref{tab:pcs1} we report the increase in $\bar{R}^2$ when the
macro variables are added to a predictive regression of annual bond
returns with $N \in \{3, 4, 5\}$ PCs of yields. We report 95\%-bootstrap
intervals to gauge how large of an increase in $\bar{R}^2$ would be
plausible under the null hypothesis. We find that $N$ does not affect
the findings we reported in the paper: with
only the exception of the original CPO sample, the increases in $\bar{R}^2$
are within the bootstrap intervals, suggesting that these increases
are perfectly consistent with the spanning hypothesis.

\begin{table}
\caption{Tests of alternative spanning hypotheses}
\label{tab:pcs2}\centering
\vspace{.5pc}
\begin{tabular}{llcccccc}
\toprule
&& \multicolumn{3}{c}{Original sample period} & \multicolumn{3}{c}{Later sample: 1985--2016} \\
\cmidrule(lr{.5em}){3-5} \cmidrule(lr{.5em}){6-8}
&& $N=3$ & $N=4$ & $N=5$ & $N=3$ & $N=4$ & $N=5$ \\
\midrule
  \input{../../tables/pcs2.tex}
  \bottomrule
\end{tabular}%
\par
\begin{flushleft}
  {\small Conventional and bootstrap tests of different null hypotheses that
    adding macro variables to a regression with $N$ PCs of bond yields
    does not increase the predictive power for annual excess bond
    returns. The bootstrap procedure is described in the text.}
\end{flushleft}
\end{table}

In Table \ref{tab:pcs2} we consider Wald tests (for JPS and LN, using
HAC standard errors) and
$t$-tests (for CPO, using the RR approach) of the different spanning hypotheses. We report
$p$-values of these tests using the conventional asymptotic
distributions, estimates of the size of these tests based
on the small-sample bootstrap distributions of the test statistics, and the
bootstrap (i.e., size-corrected) $p$-values. The true size of the
conventional five-percent tests of the spanning hypothesis is estimated to
be between 32 and 52 percent. The bootstrap $p$-values, which account
for these enormous size distortions, are therefore much higher then
conventional $p$-values, and
often above five percent. Few noticeable differences in the bootstrap
$p$-values arise from raising the
number of yield PCs to four or five---in the original JPS
sample the $p$-values increase and in the later LN sample they
decrease. Overall, our conclusions about the robustness of published
rejections of the spanning hypothesis remain unchanged when we
consider versions of the spanning hypothesis with four or five
PCs instead of our baseline hypothesis where three PCs span the
information in the yield curve.

\section{Additional results for Joslin-Priebsch-Singleton}
\label{app:jps}

In Table \ref{tab:app-jps} we show additional results for the
$\bar{R}^2$ in predictive regressions with three yield PCs and the
macro variables $GRO$ and $INF$ proposed by \cite{jps}. The dependent
variables are the annual excess returns for bonds with maturity from
two to ten years. That is, Table \ref{tab:app-jps} reports the same results for each
individual bond which Table \ref{tab:r2} reports in its
top panel for the average excess return across bond
maturities. To economize on space we only show the bootstrap results for the bias-corrected
(BC) bootstrap.

The results in Table \ref{tab:app-jps} show that the increase in $\bar{R}^2$ when macro variables are added
is often large although the spanning hypothesis is true in
population. While for the two- to four-year bonds, the increase in
$\bar{R}^2$ in the data is larger than the upper bound of the
95\%-bootstrap interval, for the remaining bonds this statistic is
within this interval, meaning that there is no statistical evidence against the
spanning hypothesis.

\begin{table}
\caption{Joslin-Priebsch-Singleton: $\bar{R}^2$ for excess-return
  regressions}
\small
\label{tab:app-jps}\centering
\vspace{.5pc}
\begin{changemargin}{-2.5pc}
\begin{tabular}{llcccccc}
\toprule
&& \multicolumn{3}{c}{Original sample: 1985--2008} & \multicolumn{3}{c}{Later sample: 1985--2016} \\
\cmidrule(lr{.25em}){3-5} \cmidrule(lr{.25em}){6-8}
&& $\bar{R}^2_1$ & $\bar{R}^2_2$ & $\bar{R}^2_2-\bar{R}^2_1$ & $\bar{R}^2_1$
& $\bar{R}^2_2$ & $\bar{R}^2_2-\bar{R}^2_1$ \\
\midrule
\emph{Two-year} &
Data                       & 0.14  & 0.48  & 0.34                        & 0.13  & 0.26  & 0.13   \\
\emph{bond} & BC bootstrap & 0.45 & 0.51 & 0.06                          & 0.37 & 0.41 & 0.05  \\
                         & & (0.10, 0.78) & (0.15, 0.80) & (0.00, 0.20)  & (0.09, 0.65) & (0.14, 0.68) & (0.00, 0.17)  \\
\midrule
\emph{Three-year} &
Data                       & 0.12  & 0.41  & 0.29                        & 0.10  & 0.22  & 0.12   \\
\emph{bond} & BC bootstrap & 0.39 & 0.45 & 0.06                          & 0.31 & 0.36 & 0.05  \\
            &              & (0.07, 0.72) & (0.13, 0.75) & (0.00, 0.21)  & (0.07, 0.59) & (0.11, 0.62) & (0.00, 0.19)  \\
\midrule
\emph{Four-year} &
Data                       & 0.14  & 0.40  & 0.26                        & 0.12  & 0.20  & 0.08   \\
\emph{bond} &BC bootstrap  & 0.38 & 0.44 & 0.06                          & 0.30 & 0.36 & 0.05  \\
             &             & (0.08, 0.69) & (0.14, 0.72) & (0.00, 0.22)  & (0.06, 0.57) & (0.11, 0.60) & (0.00, 0.19)  \\
\midrule
\emph{Five-year} &
Data                       & 0.15  & 0.38  & 0.22                        & 0.14  & 0.20  & 0.06   \\
\emph{bond} &BC bootstrap  & 0.35 & 0.41 & 0.06                          & 0.28 & 0.33 & 0.06  \\
             &             & (0.08, 0.65) & (0.14, 0.69) & (0.00, 0.23)  & (0.06, 0.54) & (0.10, 0.58) & (0.00, 0.20)  \\
\midrule
\emph{Six-year} &
Data                       & 0.18  & 0.39  & 0.21                        & 0.16  & 0.21  & 0.05   \\
\emph{bond} &BC bootstrap  & 0.37 & 0.43 & 0.06                          & 0.28 & 0.34 & 0.05  \\
             &             & (0.10, 0.65) & (0.15, 0.69) & (0.00, 0.21)  & (0.06, 0.52) & (0.10, 0.57) & (0.00, 0.19)  \\
\midrule
\emph{Seven-year} &
Data                       & 0.18  & 0.37  & 0.18                        & 0.17  & 0.21  & 0.04   \\
\emph{bond} &BC bootstrap  & 0.33 & 0.39 & 0.06                          & 0.27 & 0.32 & 0.05  \\
            &              & (0.07, 0.59) & (0.13, 0.64) & (0.00, 0.23)  & (0.05, 0.51) & (0.09, 0.55) & (0.00, 0.20)  \\
\midrule
\emph{Eight-year} &
Data                       & 0.20  & 0.37  & 0.17                        & 0.18  & 0.22  & 0.04   \\
\emph{bond} &BC bootstrap  & 0.33 & 0.39 & 0.06                          & 0.26 & 0.32 & 0.05  \\
             &             & (0.08, 0.58) & (0.13, 0.63) & (0.00, 0.22)  & (0.06, 0.50) & (0.11, 0.55) & (0.00, 0.20)  \\
\midrule
\emph{Nine-year} &
Data                       & 0.22  & 0.39  & 0.16                        & 0.19  & 0.23  & 0.03   \\
\emph{bond} &BC bootstrap  & 0.34 & 0.40 & 0.06                          & 0.27 & 0.32 & 0.05  \\
             &             & (0.10, 0.58) & (0.15, 0.64) & (0.00, 0.22)  & (0.07, 0.49) & (0.11, 0.54) & (0.00, 0.20)  \\
\midrule
\emph{Ten-year} &
Data                       & 0.20  & 0.36  & 0.15                        & 0.19  & 0.24  & 0.04   \\
\emph{bond} &BC bootstrap  & 0.31 & 0.37 & 0.06                          & 0.28 & 0.33 & 0.05  \\
             &             & (0.07, 0.56) & (0.12, 0.61) & (0.00, 0.23)  & (0.07, 0.51) & (0.11, 0.55) & (0.00, 0.19)  \\
\bottomrule
\end{tabular}%
\end{changemargin}
\par
\begin{flushleft}
  {\small $\bar{R}^2$ for regressions of annual excess bond returns on
    three PCs of the yield curve ($\bar{R}^2_1$) and on three yield PCs together
    with the macro variables $GRO$ and $INF$ ($\bar{R}^2_2$), as well
    as the increase in $\bar{R}^2$. The macro data is
    described in the text. The results in the left half of the table
    are for the original sample period of \cite{jps}; the data used in the right half
    is extended to December 2016. Each panel reports first the
    statistics in the data, and then the mean and the 95\%-bootstrap
    intervals (in parentheses) of the bootstrap small-sample
    distribution. The bootstrap, which is explained in the text,
    imposes the null hypothesis that the macro variables have no
    predictive power.}
\end{flushleft}
\end{table}

\section{Additional results for Ludvigson-Ng}
\label{app:ln}

LN also constructed a single return-forecasting factor using a similar
approach as \cite{cp05}. They regressed the excess bond returns, averaged
across the two- through five-year maturities, on the macro factors plus a
cubed term of $F1$ which they found to be important. The fitted values of this
regression produced their return-forecasting factor, denoted by $H8$. Adding
$H8$ to a predictive regression that includes the Cochrane-Piazessi factor
$CP$ substantially increases the $\bar{R}^{2}$, and leads to a highly
significant coefficient on $H8$. LN emphasized this result and interpreted it
as further evidence that macro variables have predictive power beyond the
information in the yield curve.

\begin{table}
\caption{Ludvigson-Ng: $\bar{R}^2$ for regressions with return-forecasting factors}
\label{tab:ln2}\centering
\vspace{.5pc}
\begin{changemargin}{-1pc}
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{Original sample: 1964--2007} & \multicolumn{3}{c}{Later sample: 1985--2016} \\
\cmidrule(lr{.5em}){2-4} \cmidrule(lr{.5em}){5-7}
& $\bar{R}^2_1$ & $\bar{R}^2_2$ & $\bar{R}^2_2-\bar{R}^2_1$ & $\bar{R}^2_1$
& $\bar{R}^2_2$ & $\bar{R}^2_2-\bar{R}^2_1$ \\
\midrule
\emph{Two-year bond} \\
Data                        & 0.31  & 0.42  & 0.11                       & 0.16  & 0.22  & 0.06                             \\
Bootstrap                   & 0.21 & 0.24 & 0.03                         & 0.31 & 0.35 & 0.04                               \\
                            & (0.06, 0.39) & (0.09, 0.41) & (0.00, 0.11) & (0.09, 0.54) & (0.14, 0.56) & (0.00, 0.13)     \\
\emph{Three-year bond} \\
Data                        & 0.33  & 0.43  & 0.10                       & 0.16  & 0.22  & 0.07                            \\
Bootstrap                   & 0.20 & 0.24 & 0.04                         & 0.29 & 0.33 & 0.04                              \\
                            & (0.06, 0.38) & (0.09, 0.41) & (0.00, 0.11) & (0.08, 0.51) & (0.14, 0.54) & (0.00, 0.14)    \\
\emph{Four-year bond} \\
Data                        & 0.36  & 0.45  & 0.09                       & 0.19  & 0.26  & 0.07                            \\
Bootstrap                   & 0.21 & 0.25 & 0.04                         & 0.30 & 0.34 & 0.04                              \\
                            & (0.07, 0.39) & (0.10, 0.42) & (0.00, 0.11) & (0.10, 0.52) & (0.15, 0.54) & (0.00, 0.13)    \\
\emph{Five-year bond} \\
Data                        & 0.33  & 0.42  & 0.09                       & 0.18  & 0.24  & 0.06                          \\
Bootstrap                   & 0.21 & 0.24 & 0.04                         & 0.29 & 0.32 & 0.04                            \\
                            & (0.06, 0.39) & (0.10, 0.41) & (0.00, 0.11) & (0.09, 0.50) & (0.14, 0.53) & (0.00, 0.14)  \\
\bottomrule
\end{tabular}%
\end{changemargin}
\par
\begin{flushleft}
  {\small $\bar{R}^2$ for regressions of annual excess bond returns
    on yield and macro factors, as in
    \cite{ludvigson-ng-handbook}. $\bar{R}^2_1$ is for regressions
    with only the return-forecasting factor based on yield-curve
    information ($CP$), $\bar{R}^2_2$ is for regressions that also
    include the return-forecasting factor based on macro information
    ($H8$). The left side of the table shows results for the original data
    set used by \cite{ludvigson-ng-handbook}, and the right side shows results for a data sample
    that starts in 1985 and ends in 2016. We report the values of the statistics in the data,
    and the means and 95\%-bootstrap intervals (in parentheses) for
    the bootstrap small-sample distributions, obtained under the
    null hypothesis that the macro variables have no predictive
    power. The bootstrap procedure is described in the text.}
\end{flushleft}
\end{table}

Tables \ref{tab:ln2} and \ref{tab:ln3} replicate LN's results for these
regressions on the macro- ($H8$) and yield-based ($CP$) return-forecasting
factors.\footnote{These results correspond to those in column 9 in tables 4-7
in LN.} Table \ref{tab:ln2} shows coefficient estimates and statistical
significance, while Table \ref{tab:ln3} reports $\bar{R}^{2}$. In LN's data,
both $CP$ and $H8$ are strongly significant with HAC $p$-values below 0.1\%.
Adding $H8$ to the regression increases the $\bar{R}^{2}$ by 9-11
percentage points.

One advantage of our bootstrap approach is that we can calculate the
small-sample properties under the null hypothesis of complicated
transformations of the original data such as these. \ To this end, we simply
add an additional step in the construction of our artificial data by
calculating $CP$ and $H8$ in each bootstrap data set as the fitted values from
preliminary regressions in the exact same way that LN did in the actual data.

\begin{table}
\caption{Ludvigson-Ng: statistical inference in regressions with return-forecasting factors}
\label{tab:ln3}\centering
\vspace{.5pc}
\begin{tabular}{lccccccccc}
  \toprule
  & \multicolumn{2}{l}{Two-year bond}& \multicolumn{2}{l}{Three-year bond}& \multicolumn{2}{l}{Four-year bond}& \multicolumn{2}{l}{Five-year bond} \\
  & $CP$ & $H8$ & $CP$ & $H8$ & $CP$ & $H8$ & $CP$ & $H8$ \\
  \midrule
  \multicolumn{5}{l}{\emph{Original sample: 1964--2007}} &  &  &  &  &  \\
  Coefficient & 0.335 & 0.331 & 0.645 & 0.588 & 0.955 & 0.776 & 1.115 & 0.937 \\
  HAC $t$-statistic & 4.429 & 4.331 & 4.666 & 4.491 & 4.765 & 4.472 & 4.371 & 4.541 \\
  HAC $p$-value & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} \\
  Bootstrap 5\% c.v. &  & 3.857 &  & 3.968 &  & 3.965 &  & 3.998 \\
  Bootstrap $p$-value &  & \textbf{0.019} &  & \textbf{0.021} &  & \textbf{0.023} &  & \textbf{0.019} \\
  \multicolumn{5}{l}{\emph{Size}} &  &  &  &  &  \\
  \quad HAC &  & 0.579 &  & 0.612 &  & 0.610 &  & 0.594 \\
  \quad Bootstrap &  & 0.049 &  & 0.059 &  & 0.054 &  & 0.049 \\
  \multicolumn{5}{l}{\emph{Power}} &  &  &  &  &  \\
  \quad Bootstrap &  & 0.621 &  & 0.573 &  & 0.555 &  & 0.521 \\
  \midrule
  \multicolumn{5}{l}{\emph{Later sample: 1985--2016}} &  &  &  &
  &  \\
Coefficient & 0.363 & 0.333 & 0.678 & 0.663 & 1.101 & 0.934 & 1.314 & 1.146 \\
  HAC statistic & 2.746 & 2.768 & 2.556 & 3.073 & 2.933 & 3.308 & 2.837 & 3.379 \\
  HAC $p$-value & \textbf{0.006} & \textbf{0.006} & \textbf{0.011} & \textbf{0.002} & \textbf{0.004} & \textbf{0.001} & \textbf{0.005} & \textbf{0.001} \\
  Bootstrap 5\% c.v. &  & 4.182 &  & 4.172 &  & 4.158 &  & 4.160 \\
  Bootstrap $p$-value &  & 0.271 &  & 0.199 &  & 0.153 &  & 0.134 \\
  \bottomrule
\end{tabular}%
\par
\begin{flushleft}
  {\small Predictive regressions for annual excess bond returns, using
    return-forecasting factors based on yield-curve information ($CP$)
    and macro information ($H8$), as in
    \cite{ludvigson-ng-handbook}. The first panel shows the results
    for their original data and sample period; the second panel uses a
    data sample that starts in 1985 and ends in 2016. HAC
    $t$-statistics and $p$-values are calculated using Newey-West
    standard errors with 18 lags. We obtain bootstrap small-sample
    distributions of the $t$-statistics under the null hypothesis that
    macro factors and hence $H8$ have no predictive power, and report
    the bootstrap critical values (c.v.'s) and $p$-values, as well as
    estimates of the true size of conventional HAC $t$-tests and the
    bootstrap tests with 5\% nominal coverage (see notes to Table
    \ref{tab:jps}). We also report estimates of the power of the
    bootstrap tests. The bootstrap procedure is described in the
    text. $p$-values below 5\% are emphasized with bold face.}
\end{flushleft}
\end{table}

Table \ref{tab:ln2} shows that the observed increases in $\bar{R}^{2}$
when adding $H8$ to the regression are generally within the 95\%
bootstrap intervals. That is, although LN find large
increases in $\bar{R}^{2}$ using these same regression specifications,
this is not convincing evidence against the spanning hypothesis, as
such increases in goodness-of-fit are perfectly plausible under the
null hypothesis. And according to the bootstrap $p$-values for the
coefficients on $H8$ in Table \ref{tab:ln3}, the macro
return-forecasting factor is no longer significant at the 1\%
level. Furthermore, the size distortions for conventional $t$-tests
are very substantial: a test with nominal size of five percent based
on asymptotic HAC $p$-values has a true size of 58-61 percent.  This
evidence suggests that conventional HAC inference can be particularly
problematic when the predictors are return-forecasting factors. Table
\ref{tab:ln3} also shows that the bootstrap test has good size and
power.

We also examined the same regressions over the 1985--2016 sample period with
results shown in the right half
of Table \ref{tab:ln2} and in the bottom panel of Table \ref{tab:ln3}. The observed increases in $\bar{R}^{2}$ are squarely in
line with what we would expect under the spanning hypothesis, as indicated by
the bootstrap intervals in Table \ref{tab:ln2}. The return-forecasting factors would
again appear to be highly significant based on HAC $p$-values, but the
size distortions of these tests are again very substantial and the
coefficients on $H8$ are in fact not statistically significant when using the
bootstrap $p$-values.

This evidence suggests that conventional HAC inference can be particularly
problematic when the predictors are return-forecasting factors. One reason for
the substantially distorted inference is their high persistence; $H8$ and $CP$ have autocorrelations that are
around 0.8, and decline only slowly with the lag length. Another
reason is that the return-forecasting factors are constructed in a preliminary
estimation step, which introduces additional estimation uncertainty not
accounted for by conventional inference. We recommend that researchers
use our bootstrap in such a setting to accurately carry out
inference. Here we conclude that LN's macro return-forecasting factor
exhibits only very tenuous predictive power, much weaker than indicated by
LN's original analysis, which disappears completely over a
different sample period.

\section{Bond supply: Greenwood-Vayanos}
\label{app:gv}

A large literature studies the
effects of the supply of bonds on prices and yields, including the
recent contributions of
\cite{hamilton-wu} and \cite{greenwood-vayanos}. Theoretical
and empirical work has demonstrated that bond supply is related
to bond yields and returns. But do measures of Treasury bond supply contain
predictive power for bond returns that is not already reflected
in the yield curve? The answer appears to be yes:
\cite{greenwood-vayanos} (henceforth GV) found that variation in their measure of bond
supply, a maturity-weighted debt-to-GDP ratio, predicts yields and bond
returns, and that this holds true even controlling for yield curve information
such as the term spread. Here we investigate whether this result is
robust and holds up to
closer scrutiny. The sample period used in GV is 1952 to
2008.

\begin{table}
  \centering
  \caption{Greenwood-Vayanos: predictive power of Treasury bond supply}
  \label{tab:gv}\vspace{.5pc}
  \begin{tabular}{lcccccc}
 \toprule
    & One-year & Term & & & & Bond  \\
    & yield & spread &$PC1$ & $PC2$ & $PC3$ & supply \\
\midrule
    \multicolumn{6}{l}{\emph{Dependent variable: return on long-term bond }} \\
    Coefficient & 1.212 &  &  &    &  & 0.026 \\
    HAC $t$-statistic & 2.853 &    &  &  &  & 3.104 \\
    HAC $p$-value & \textbf{0.004} &  &    &  &  & \textbf{0.002} \\
\midrule
    \multicolumn{6}{l}{\emph{Dependent variable: return on long-term bond }} \\
    Coefficient & 1.800 & 2.872 &  &  &    & 0.014 \\
    HAC $t$-statistic & 5.208 & 4.596 &    &  &  & 1.898 \\
    HAC $p$-value & \textbf{0.000} & \textbf{0.000} &  &    &  & 0.058 \\
\midrule
    \multicolumn{6}{l}{\emph{Dependent variable: excess return on long-term bond }} \\
    Coefficient &  &  &   0.168 & 5.842 & -6.089 & 0.013 \\
    HAC $t$-statistic &  &  &   1.457 & 4.853 & 1.303 & 1.862 \\
    HAC $p$-value &  &  &   0.146 & \textbf{0.000} & 0.193 & 0.063 \\
\midrule
    \multicolumn{6}{l}{\emph{Dependent variable: avg.~excess return for 2-5 year bonds}} \\
    Coefficient   &  &  & 0.085 & 1.669 & -4.632 & 0.004 \\
    HAC statistic &  &  & 1.270 & 3.156 & 2.067 & 1.154 \\
    HAC $p$-value &  &  & 0.204 & \textbf{0.002} & \textbf{0.039} & 0.249 \\
    Bootstrap 5\% c.v. &  &  &  &  &  & 3.199 \\
    Bootstrap $p$-value &  &  &  &  &  & 0.468 \\
\bottomrule
  \end{tabular}
  \par
  \begin{flushleft}
    \small Predictive regressions for annual bond returns using
    Treasury bond supply, as in \cite{greenwood-vayanos} (GV). The
    coefficients on bond supply in the first two panels are identical
    to those reported in rows (1) and (6) of Table 5 in GV. HAC
    $t$-statistics and $p$-values are constructed using Newey-West
    standard errors with 36 lags, as in GV. The last panel includes
    bootstrap critical values and $p$-values using small-sample
    distributions generated under the null hypothesis that bond supply
    does not contain additional predictive power---the bootstrap
    procedure is described in the text. The last two rows in each
    panel report $p$-values for $t$-tests using the methodology of
    \cite{ibragimov-mueller}, splitting the sample into either 8 or 16
    blocks. The sample period is 1952 to 2008. $p$-values below 5\%
    are emphasized with bold face.
  \end{flushleft}
\end{table}

We are most interested in those regression specifications estimated by GV that
control for the information in the yield curve. We first reproduce, in the
top panel of Table \ref{tab:gv}, their baseline specification in
which the one-year return on a long-term bond is predicted using the one-year
yield and bond supply measure alone. The second panel includes the spread
between the long-term and one-year yield as an additional explanatory
variable.\footnote{These estimates are in GV's table 5, rows 1 and 6. Their
baseline results are also in their table 2.} Like GV we use Newey-West
standard errors with 36 lags. If we interpreted the HAC $t$-test using the conventional asymptotic critical
values, the coefficient on bond supply is significant in the baseline
regression in the top panel. When the yield spread is included in the
regression, this coefficient is marginally insignificant, with a
$p$-value of 5.8\%.

The bond return that GV used as the dependent variable in these regressions is
for a hypothetical long-term bond with a 20-year maturity. We cannot apply our
bootstrap procedure here because this bond return is not constructed from the
observed yield curve.\footnote{GV obtained this series from Ibbotson
  Associates.}

We consider two additional regression specifications that are relevant in this
context. The first specification controls for information in the yield curve by including,
instead of a single term spread, the first three PCs of observed
yields.\footnote{These PCs are calculated from the observed Fama-Bliss yields
with one- through five-year maturities.} It also subtracts the one-year yield
from the bond return in order to yield an excess return. Both of these changes
make this specification more closely comparable to those in the literature.
The results are reported in the third panel of Table \ref{tab:gv}. Again, the
coefficient on bond supply is only marginally significant for the HAC
$t$-test.

Finally, we consider a specification where the one-year excess return,
averaged across two- though five-year maturities, is regressed on
yield PCs and the measure of bond supply. The last panel of Table
\ref{tab:gv} shows that in this case, the coefficient on bond supply
is insignificant according to the conventional Newey-West
$t$-test. In
this last regression, which includes PCs and a conventional excess
bond return, we can also use our bootstrap procedure. We find that the
bootstrap $p$-value is substantially higher than the conventional
$p$-value. The bond supply variable has a first-order autocorrelation
is 0.998, which causes substantial size distortions for the
conventional $t$-test in this and in the other regression
specifications.

Overall, we find that the results in GV do not constitute evidence against the spanning
hypothesis. While bond supply exhibits a strong empirical link with interest
rates, its predictive power for future yields and returns seems to be fully
captured by the current yield curve.

\section{Output gap: Cooper-Priestley}
\label{app:cpr}

\begin{table}
  \centering
  \caption{Cooper-Priestley: predictive power of the output gap}
  \label{tab:cpr}\vspace{.5pc}
  \begin{tabular}{lrrrrrrrrrrr}
    \toprule
    & $gap$ & $\tilde{CP}$ & $CP$ & $PC1$ & $PC2$ & $PC3$ \\
    \midrule
    Coefficient & -0.126 &  &  &  &  &  \\
    OLS $t$-statistic & 3.225 &  &  &  &  &  \\
    HAC $t$-statistic & 1.077 &  &  &  &  &  \\
    HAC $p$-value & 0.282 &  &  &  &  &  \\
    \midrule
    Coefficient & -0.120 & 1.588 &  &  &  &  \\
    OLS $t$-statistic & 3.479 & 13.541 &  &  &  &  \\
    HAC $t$-statistic & 1.244 & 4.925 &  &  &  &  \\
    HAC $p$-value & 0.214 & \textbf{0.000} &  &  &  &  \\
    \midrule
    Coefficient & 0.113 &  & 1.612 &  &  &  \\
    OLS $t$-statistic & 2.940 &  & 13.831 &  &  &  \\
    HAC $t$-statistic & 1.099 &  & 5.059 &  &  &  \\
    HAC $p$-value & 0.273 &  & \textbf{0.000} &  &  &  \\
    \midrule
    Coefficient & 0.147 &  &  & 0.001 & 0.043 & -0.067 \\
    OLS $t$-statistic & 3.524 &  &  & 4.359 & 11.506 & 3.690 \\
    HAC $t$-statistic & 1.306 &  &  & 1.332 & 4.363 & 2.508 \\
    HAC $p$-value & 0.192 &  &  & 0.183 & \textbf{0.000} & 0.012 \\
    Bootstrap 5\% c.v. & 2.843 &  &  &  &  &  \\
    Bootstrap $p$-value & 0.354 &  &  &  &  &  \\
    \bottomrule
  \end{tabular}
  \par
  \begin{flushleft}
    \small Predictive regressions for the one-year excess return on a
    five-year bond using the output gap, as in \cite{cooper-priestley}
    (CPR). $\tilde{CP}$ is the Cochrane-Piazzesi factor after
    orthogonalizing it with respect to $gap$, whereas $CP$ is the
    usual Cochrane-Piazzesi factor. For the predictive regression,
    $gap$ is lagged one month, as in CPR. HAC standard errors are
    based on the Newey-West estimator with 22 lags. The bootstrap
    procedure, which does not include bias correction, is described in
    the main text. The sample period is 1952 to 2003. $p$-values below
    5\% are emphasized with bold face.
  \end{flushleft}
\end{table}

Another widely cited study that appears to provide evidence of predictive
power of macro variables for asset prices is \cite{cooper-priestley}
(henceforth CPR). This paper focuses on one particular macro variable as a
predictor of stock and bond returns, namely the output gap, which is a key
indicator of the economic business cycle. The authors concluded that
\textquotedblleft the output gap can predict next year's excess returns on
U.S.~government bonds\textquotedblright\ (p.~2803). Furthermore, they also
claimed that some of this predictive power is independent of the information
in the yield curve, and implicitly rejected the spanning hypothesis (p.~2828).

Like CPR we use $x_{2t}=gap_{t-1}$, the output gap at date $t-1$, measured as the deviation of
the Fed's Industrial Production series from a quadratic time
trend.\footnote{We thank Richard Priestley for sending us this real-time
measure of the output gap.} CPR lagged their measure by one month to account
for the publication lag of the Fed's Industrial Production data. Table
\ref{tab:cpr} shows our results for predictions of the excess return on the
five-year bond; the results for other maturities closely parallel these. The
top two panels correspond to the regression specifications that CPR
estimated.\footnote{The relevant results in CPR are in the top panel of their
table 9.} In the first specification, the only predictor is $gap_{t-1}$. The
second specification also includes $\tilde{CP}_{t}$, which is the
Cochrane-Piazzesi factor $CP_{t}$ after it is orthogonalized with respect to
$gap_{t}$.\footnote{Note that the predictors $\tilde{CP}_{t}$ and $gap_{t-1}$
are therefore not completely orthogonal.} We obtain coefficients and $\bar
{R}^{2}$ that are close to those published in CPR. We calculate both OLS and
HAC $t$-statistics, where in the latter case we use Newey-West with 22 lags as
described by CPR. Our OLS $t$-statistics are very close to the published
numbers, and according to these the coefficient on $gap_{t-1}$ is highly
significant. It appears that CPR may
have mistakenly reported the OLS instead of the Newey-West
$t$-statistics, which is about a third as large as the OLS
$t$-statistics and implies that the coefficient on $gap$ is far from
significant, with $p$-values above 20\%.

Importantly, neither of the specifications in CPR can be used to test the
spanning hypothesis, because the CP factor is first orthogonalized with
respect to the output gap. This defeats the purpose of controlling for
yield-curve information, since any predictive power that is shared by the CP
factor and $gap$ will be exclusively attributed to the latter. In
particular, finding a significant coefficient on $gap$ in a regression with
$\tilde{CP}$ cannot justify the conclusion that \textquotedblleft$gap$ is
capturing risk that is independent of the financial market-based variable
CP\textquotedblright\ (p.~2828). One way to test the spanning hypothesis is
to include $CP$ instead of $\tilde{CP}$, and we report these results in
the third panel of Table \ref{tab:cpr}. In this case, the coefficient on $gap$
switches to a positive sign, and its Newey-West $t$-statistic remains
insignificant.

Our preferred specification includes the first three PCs of the yield
curve---see the last panel of Table \ref{tab:cpr}. The predictor
$gap$ is highly persistent, with a first-order autocorrelation coefficient of
0.975, so there are likely small-sample inference problems. Hence we
also include results for robust inference
using the bootstraptest. The $gap$ variable has a positive
coefficient with a HAC $p$-value of 19\%, which rises to 36\% when using our
bootstrap procedure. The conventional HAC $t$-test is substantially oversized,
as evident by the bootstrap critical value that substantially exceeds the
conventional critical value. Overall, we do not find any  evidence that the output gap
predicts excess bond returns.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "bh_robust"
%%% End:
