## estimate simple DTSM with MLE
## - specification as in our trends paper (version June 2017)
## - goals: estimate r* from the yield curve

rm(list=ls())
source("R/fns.R")

description <- "Simple DTSM - measured r*"

## scaling: all data is in percent per year; monthly model
nper <- 12

## load yield data
df <- read.csv("data/yields_gsw_monthly.csv")
mats <- c(0.25, 0.5, 1, 2, 3, 5, 7, 10)
yield.cols <- paste0("y", mats)
df <- df[c("Date", yield.cols)]
mats <- nper*mats
df$yyyymm <- floor(df$Date/100)
df <- subset(df, yyyymm >= 197111)
df$Date <- as.Date(as.character(df$Date), format="%Y%m%d")

## load inflation
cpi <- read.csv("data/CPILFENS.csv")
cpi$DATE <- as.Date(cpi$DATE, format="%Y-%m-%d")
cpi$yyyymm <- as.numeric(format(cpi$DATE, "%Y%m"))
cpi$pi <- c(rep(NA, 12), diff(log(cpi$CPILFENS), 12)) # year-over-year inflation
cpi$DATE <- NULL; cpi$CPILFENS <- NULL
df <- merge(df, cpi, all.x=TRUE)
df$pi <- df$pi*100 ## per year, percent

## load PTR
ptr <- read.csv("data/ptrcpi.csv")
ptr$date <- as.Date(ptr$date, format="%m/%d/%Y")
ptr$yyyymm <- as.numeric(format(ptr$date, "%Y%m")) + 1 # middle month of the quarter
ptr$date <- NULL
names(ptr) <- c("ptr", "yyyymm")
df <- merge(df, ptr, all.x=TRUE) # contains missing values

## load r-star
df <- merge(df, loadLW(), all.x=TRUE)
df <- merge(df, loadLM(), all.x=TRUE)
df <- merge(df, loadKiley(), all.x=TRUE)

scale <- 1
theta2pars <- function(theta) {
    ## Globals: scale, nper
    pars <- list(Sigma = diag(scale*c(exp(theta[1:5]))), # 5 shock variances
                 sig2 = scale*exp(theta[6:11]),  # 6 meas error variances -- inflation, yields, pi*, 3 x r*
                 mu = c(0, 0, 0, 0, scale*theta[12]),
                 phi = diag(c(1, theta[13]^2/(1+theta[13]^2), 1, theta[14]^2/(1+theta[14]^2),
                                 theta[15]^2/(1+theta[15]^2))),
                 lam0 = matrix(scale*c(theta[16:19], 0), 5, 1),
                 lam1 = scale*cbind(matrix(0, 5, 4), c(theta[20:23], 0)))
    pars$Omega <- pars$Sigma %*% t(pars$Sigma)
    pars$delta0 <- -.5*pars$sig2[1]/(100*nper)^2
    pars$delta1 <- c(1,1,1,1,0)/100/nper
    pars
}

pars2theta <- function(pars)
    ## Globals: scale
    c(log(diag(pars$Sigma)/scale), log(pars$sig2/scale),
      pars$mu[5]/scale,
      sqrt(diag(pars$phi)[c(2,4,5)]/(1-diag(pars$phi)[c(2,4,5)])),
      pars$lam0[1:4]/scale, pars$lam1[1:4,5]/scale)
      ## pars$lam0[c(1,3)]/scale, pars$lam1[c(1,3),5]/scale) # only r* and pi* risk priced

affineLoadings <- function(mu, phi, lam0, lam1, Omega, delta0, delta1) {
    ## Globals: mats, nper
    J <- length(mats)
    A <- matrix(NA, 1, J)
    B <- matrix(NA, length(delta1), J)
    Atmp <- 0
    Btmp <- rep(0, 5)
    muQ <- mu - lam0
    phiQ <- phi - lam1
    phiQp <- t(phiQ)
    for (n in 1:max(mats)) {
        Atmp <- Atmp + crossprod(muQ, Btmp) + .5 * t(Btmp) %*% Omega %*% Btmp - delta0
        Btmp <- phiQp %*% Btmp - delta1
        ind <- mats==n
        if (any(ind)) {
            A[which(ind)] <- - Atmp/n
            B[,which(ind)] <- - Btmp/n
        }
    }
    list(A=100*nper*A, B=100*nper*B)  ## yields in percent per year
}

kalmanDTSM <- function(pars, smooth=FALSE) {
    ## globals: mats, df, yield.cols
    loads <- affineLoadings(pars$mu, pars$phi, pars$lam0, pars$lam1, pars$Omega, pars$delta0, pars$delta1)
    ## measurement equation: J yields, inflation, PTR, r*
    J <- length(mats)
    M <- length(pars$sig2)-1
    ct <- matrix(c(loads$A, rep(0, M)), J+M, 1)
    Zt <- array(rbind(t(loads$B),       # yields
                      c(1,1,0,0,0),  # inflation
                      c(1,0,0,0,0),  # PTR
                      c(0,0,1,0,0),  # r* LW
                      c(0,0,1,0,0),  # r* LM
                      c(0,0,1,0,0)),  # r* kiley
                c(J+M, 5, 1))
    GGt <- array(diag(c(rep(pars$sig2[2], J), unlist(pars$sig2[-2]))), c(J+M, J+M, 1))
    ## state equation
    dt <- matrix(pars$mu, 5, 1)
    Tt <- array(pars$phi, c(5, 5, 1))
    HHt <- array(pars$Omega, c(5, 5, 1))
    ## initial conditions
    a0 <- rep(0, 5)
    P0 <- 100*diag(5)
    ## data
    yt <- rbind(t(df[yield.cols]), c(tail(df$pi, -1), NA), df$ptr, df$rstar.lw, df$rstar.lm, df$rstar.kiley) # can deal with missing values
    ## kalman filter
    if (smooth) {
        fkf.R(a0=a0, P0=P0, dt=dt, ct=ct, Tt=Tt, Zt=Zt, HHt=HHt, GGt=GGt, yt=yt, smooth=TRUE)
    } else {
        require(FKF)
        rval <- fkf(a0=a0, P0=P0, dt=dt, ct=ct, Tt=Tt, Zt=Zt, HHt=HHt, GGt=GGt, yt=yt)
        if (!(isTRUE(all.equal(rval$status, c(0,0))))) {
            stop("unsuccessful call to fkf()\n")
        } else {
            rval
        }
    }
}

obj <- function(theta) {
    pars <- theta2pars(theta)
    rval <- 1e6
    try(rval <- -kalmanDTSM(pars)$logLik, silent=TRUE)
    rval
}


goodStartingValues <- function(n=1000) {
    ## evaluate LLK at n random starting values
    ## return best ones
    startingValues <- function() {
        rho0 <- runif(3, 0.8, .99)
        dSigma <- rbeta(5, 1, 3)  # between 0 and 1, much mass close to zero
        sig <- c(runif(1, .5, 1), runif(2, .05, .15), runif(3, 0, 2))
        c(log(dSigma/scale), log(sig^2/scale),
          rnorm(1)/100, # mu_x
          log(rho0/(1-rho0)),
          rnorm(8)/100)
          ## rnorm(4)/100) # lam0, lam1 -- only r* and pi* risk priced
    }
    thetas <- replicate(n, startingValues())
    negllks <- apply(thetas, 2, obj)
    thetas[,which.min(negllks)]
}

theta <- goodStartingValues()
pars <- theta2pars(theta)
stopifnot(isTRUE(all.equal(theta, pars2theta(pars))))
cat("Some good starting values give LLK =", -obj(theta), "\n")
cat("check max:", kalmanDTSM(pars)$logLik, "\n")

doOptim <- function(...) {
    theta0 <- goodStartingValues()
    llk0 <- -obj(theta0)
    cat('LLK at starting point:', llk0, '\n')
    i <- 1; improvement <- Inf; llk <- llk0
    theta <- theta0
    while (improvement>.1) {
        res <- optim(theta, obj, control=list(maxit=5000))
        improvement <- -res$value - llk
        llk <- -res$value
        theta <- res$par
        pars <- theta2pars(theta)
        cat('iteration ', i,', likelihood = ', llk,'\n')
        i <- i + 1
    }
    cat('improvement = ', improvement, ' -- proceed to final step\n')
    res <- optim(theta, obj, control=list(maxit=50000))
    theta <- res$par
    llk <- -res$value
    cat('final Nelder-Mead step, likelihood = ', llk, "\n")
    cat("check max:", -obj(theta), "\n")
    print(res)
    list(theta0=theta0, llk0=llk0, theta=theta, llk=llk, conv=res$convergence)
}

## optimization from random starting values IN PARALLEL
require(parallel)
ncores <- min(10, detectCores())
nstarts <- 10*ncores
cat("# Parallel optimization from", nstarts, "different starting values\n")
cl <- makeCluster(ncores)
print(cl)
clusterExport(cl, c("scale", "mats", "df", "yield.cols", "nper"))
clusterExport(cl, c("goodStartingValues", "obj", "theta2pars", "affineLoadings", "kalmanDTSM"))
rvals <- parLapply(cl, 1:nstarts, doOptim)
stopCluster(cl)

## nstarts <- 1
## rvals <- lapply(1:nstarts, doOptim)

nms <- c("starting LLK", "optimal LLK", "convergence")
tbl <- matrix(NA, nstarts, length(nms))
colnames(tbl) <- nms
for (i in seq_along(rvals))
    tbl[i, ] <- unlist(rvals[[i]][c("llk0", "llk", "conv")])
print(round(tbl[order(tbl[,2], decreasing=TRUE),], 2))

ind <- which.max(tbl[,2])
theta <- rvals[[ind]]$theta
pars <- theta2pars(theta)
cat("best LLK: ", rvals[[ind]]$llk, "\n")
cat("check max:", -obj(theta), "\n")
cat("check max:", kalmanDTSM(pars)$logLik, "\n")

## final optimization
cat("# Final optimization. LLK at starting values:", -obj(theta), "\n")
cat("# Nelder-Mead optimization...\n")
rval <- optim(theta, obj, control=list(maxit=50000))
print(rval)
theta <- rval$par
cat("likelihood at optimum:", -obj(theta), "\n")

pars <- theta2pars(theta)
loads <- affineLoadings(pars$mu, pars$phi, pars$lam0, pars$lam1, pars$Omega, pars$delta0, pars$delta1)
print(pars)
print(round(loads$A, 2))
print(round(loads$B, 2))

save(rvals, pars, theta, df, mats, yield.cols, scale, nper, theta2pars, pars2theta, kalmanDTSM, obj, affineLoadings, description, file=paste0("results/est_results_", Sys.Date(), ".RData"))


